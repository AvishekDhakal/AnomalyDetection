#!/usr/bin/env python3
"""
EDA Script for Insider Threat Detection Project

This script performs Exploratory Data Analysis (EDA) on synthetic logs
generated by the provided log generator script. It includes data loading,
univariate and bivariate analysis, time-series and geographical analysis,
statistical summaries, bias and outlier detection, and data quality checks.

Author: [Your Name]
Date: YYYY-MM-DD
"""

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from datetime import datetime
from scipy import stats
import logging

# Configure logging
logging.basicConfig(
    filename='eda.log',
    level=logging.INFO,
    format='%(asctime)s:%(levelname)s:%(message)s'
)

# -------------------- CONFIGURATION --------------------

# File paths
MASTER_LOGS_PATH = 'data/master_logs.csv'
INFERENCE_LOGS_PATH = 'data/inference_logs.csv'

# Output directories
OUTPUT_DIR = 'eda_output'
VISUALIZATIONS_DIR = os.path.join(OUTPUT_DIR, 'visualizations')
SUMMARY_DIR = os.path.join(OUTPUT_DIR, 'summaries')
os.makedirs(VISUALIZATIONS_DIR, exist_ok=True)
os.makedirs(SUMMARY_DIR, exist_ok=True)

# -------------------- DATA LOADING --------------------

def load_data(master_path: str, inference_path: str):
    """
    Loads master and inference logs into pandas DataFrames.

    Parameters:
        master_path (str): Path to master_logs.csv
        inference_path (str): Path to inference_logs.csv

    Returns:
        master_df (pd.DataFrame): DataFrame containing master logs
        inference_df (pd.DataFrame): DataFrame containing inference logs
    """
    try:
        master_df = pd.read_csv(master_path, parse_dates=['Timestamp'])
        inference_df = pd.read_csv(inference_path, parse_dates=['Timestamp'])
        logging.info("Data loaded successfully.")
        return master_df, inference_df
    except Exception as e:
        logging.error(f"Error loading data: {e}")
        raise

# -------------------- DATA INSPECTION --------------------

def initial_inspection(df: pd.DataFrame, df_name: str):
    """
    Performs initial inspection of the DataFrame.

    Parameters:
        df (pd.DataFrame): DataFrame to inspect
        df_name (str): Name identifier for the DataFrame

    Returns:
        None
    """
    logging.info(f"Initial inspection for {df_name}.")
    print(f"--- {df_name} ---")
    print(df.head())
    print("\nData Types:")
    print(df.dtypes)
    print("\nMissing Values:")
    print(df.isnull().sum())
    print("\nSummary Statistics:")
    print(df.describe(include='all'))
    print("\n")

# -------------------- UNIVARIATE ANALYSIS --------------------

def univariate_analysis(df: pd.DataFrame, output_dir: str):
    """
    Performs univariate analysis and generates visualizations.

    Parameters:
        df (pd.DataFrame): DataFrame for analysis
        output_dir (str): Directory to save visualizations

    Returns:
        None
    """
    logging.info("Starting univariate analysis.")

    # Categorical Features
    categorical_features = ['Role', 'HTTP_Method', 'HTTP_Response', 'Anomalous']
    for feature in categorical_features:
        plt.figure(figsize=(8,6))
        sns.countplot(data=df, x=feature, order=df[feature].value_counts().index)
        plt.title(f'Distribution of {feature}')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, f'univariate_{feature}.png'))
        plt.close()
        logging.info(f"Univariate bar chart for {feature} saved.")

    # User_Agent - Pie Chart
    plt.figure(figsize=(8,8))
    user_agent_counts = df['User_Agent'].value_counts()
    user_agent_counts.plot.pie(autopct='%1.1f%%', startangle=140)
    plt.ylabel('')
    plt.title('Proportion of User_Agent Types')
    plt.savefig(os.path.join(output_dir, 'univariate_User_Agent_pie.png'))
    plt.close()
    logging.info("Univariate pie chart for User_Agent saved.")

    # Numerical Features - UserID
    plt.figure(figsize=(8,6))
    sns.histplot(df['UserID'], bins=30, kde=True)
    plt.title('Distribution of UserID')
    plt.xlabel('UserID')
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'univariate_UserID_histogram.png'))
    plt.close()
    logging.info("Univariate histogram for UserID saved.")

    # Log Frequency per UserID
    user_log_counts = df['UserID'].value_counts().sort_index()
    plt.figure(figsize=(12,6))
    sns.barplot(x=user_log_counts.index, y=user_log_counts.values)
    plt.title('Number of Logs per UserID')
    plt.xlabel('UserID')
    plt.ylabel('Number of Logs')
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'univariate_Log_Frequency_per_UserID.png'))
    plt.close()
    logging.info("Univariate log frequency per UserID bar chart saved.")

# -------------------- BIVARIATE ANALYSIS --------------------

def bivariate_analysis(df: pd.DataFrame, output_dir: str):
    """
    Performs bivariate analysis and generates visualizations.

    Parameters:
        df (pd.DataFrame): DataFrame for analysis
        output_dir (str): Directory to save visualizations

    Returns:
        None
    """
    logging.info("Starting bivariate analysis.")

    # Stacked Bar Chart: Anomalous vs Normal across Roles
    role_anomaly = pd.crosstab(df['Role'], df['Anomalous'])
    role_anomaly.plot(kind='bar', stacked=True, figsize=(10,7))
    plt.title('Anomalous vs Normal Logs across Roles')
    plt.xlabel('Role')
    plt.ylabel('Number of Logs')
    plt.legend(title='Anomalous', labels=['Normal', 'Anomalous'])
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'bivariate_Anomalous_vs_Role.png'))
    plt.close()
    logging.info("Bivariate stacked bar chart for Anomalous vs Role saved.")

    # Stacked Bar Chart: HTTP_Method usage across Roles
    role_method = pd.crosstab(df['Role'], df['HTTP_Method'])
    role_method.plot(kind='bar', stacked=True, figsize=(10,7))
    plt.title('HTTP_Method Usage across Roles')
    plt.xlabel('Role')
    plt.ylabel('Number of Logs')
    plt.legend(title='HTTP_Method', bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'bivariate_HTTP_Method_vs_Role.png'))
    plt.close()
    logging.info("Bivariate stacked bar chart for HTTP_Method vs Role saved.")

    # Box Plot: Timestamp distribution across Anomalous and Normal logs
    df['Hour'] = df['Timestamp'].dt.hour
    plt.figure(figsize=(10,6))
    sns.boxplot(x='Anomalous', y='Hour', data=df)
    plt.title('Timestamp (Hour) Distribution across Anomalous and Normal Logs')
    plt.xlabel('Anomalous')
    plt.ylabel('Hour of the Day')
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'bivariate_Timestamp_Boxplot.png'))
    plt.close()
    logging.info("Bivariate box plot for Timestamp vs Anomalous saved.")

    # Correlation Heatmap for Numerical Features
    numerical_features = ['UserID']
    # Extract numerical components from IP_Address if needed
    df['IP_First_Octet'] = df['IP_Address'].apply(lambda x: int(x.split('.')[0]))
    df['IP_Second_Octet'] = df['IP_Address'].apply(lambda x: int(x.split('.')[1]))
    df['IP_Third_Octet'] = df['IP_Address'].apply(lambda x: int(x.split('.')[2]))
    df['IP_Fourth_Octet'] = df['IP_Address'].apply(lambda x: int(x.split('.')[3]))
    numerical_features.extend(['IP_First_Octet', 'IP_Second_Octet', 'IP_Third_Octet', 'IP_Fourth_Octet'])

    corr_matrix = df[numerical_features].corr()
    plt.figure(figsize=(10,8))
    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f")
    plt.title('Correlation Matrix for Numerical Features')
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'bivariate_Correlation_Heatmap.png'))
    plt.close()
    logging.info("Bivariate correlation heatmap saved.")

# -------------------- TIME-SERIES ANALYSIS --------------------

def time_series_analysis(df: pd.DataFrame, output_dir: str):
    """
    Performs time-series analysis and generates visualizations.

    Parameters:
        df (pd.DataFrame): DataFrame for analysis
        output_dir (str): Directory to save visualizations

    Returns:
        None
    """
    logging.info("Starting time-series analysis.")

    # Logs per Hour
    df['Date'] = df['Timestamp'].dt.date
    logs_per_day = df.groupby('Date').size()

    plt.figure(figsize=(12,6))
    logs_per_day.plot()
    plt.title('Number of Logs per Day')
    plt.xlabel('Date')
    plt.ylabel('Number of Logs')
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'time_series_Logs_per_Day.png'))
    plt.close()
    logging.info("Time-series line chart for Logs per Day saved.")

    # Anomalies over Time
    anomalies_per_day = df[df['Anomalous'] == 1].groupby('Date').size()
    plt.figure(figsize=(12,6))
    anomalies_per_day.plot(color='red')
    plt.title('Number of Anomalies per Day')
    plt.xlabel('Date')
    plt.ylabel('Number of Anomalies')
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'time_series_Anomalies_per_Day.png'))
    plt.close()
    logging.info("Time-series line chart for Anomalies per Day saved.")

    # Seasonal Decomposition (Optional)
    # Requires statsmodels library
    try:
        from statsmodels.tsa.seasonal import seasonal_decompose

        # Resample logs per day
        daily_counts = df.set_index('Timestamp').resample('D').size()
        decomposition = seasonal_decompose(daily_counts, model='additive', period=30)

        fig = decomposition.plot()
        fig.set_size_inches(14, 10)
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'time_series_Seasonal_Decomposition.png'))
        plt.close()
        logging.info("Seasonal decomposition plot saved.")
    except ImportError:
        logging.warning("statsmodels not installed. Skipping seasonal decomposition.")
    except Exception as e:
        logging.error(f"Error in seasonal decomposition: {e}")

# -------------------- GEOGRAPHICAL ANALYSIS --------------------

def geographical_analysis(df: pd.DataFrame, output_dir: str):
    """
    Performs geographical analysis based on IP addresses and generates visualizations.

    Parameters:
        df (pd.DataFrame): DataFrame for analysis
        output_dir (str): Directory to save visualizations

    Returns:
        None
    """
    logging.info("Starting geographical analysis.")

    # Extract IP components
    df['IP_Octets'] = df['IP_Address'].apply(lambda x: x.split('.'))
    df['IP_Octets'] = df['IP_Octets'].apply(lambda x: list(map(int, x)))
    df['IP_Combined'] = df['IP_Address']  # Placeholder for mapping if needed

    # For simplicity, we'll plot the distribution of the fourth octet
    plt.figure(figsize=(10,6))
    sns.histplot(df['IP_Octets'].apply(lambda x: x[3]), bins=30, kde=True)
    plt.title('Distribution of IP Address - Fourth Octet')
    plt.xlabel('Fourth Octet')
    plt.ylabel('Frequency')
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'geographical_IP_Fourth_Octet.png'))
    plt.close()
    logging.info("Geographical IP fourth octet distribution histogram saved.")

    # Interactive IP Mapping (Optional)
    # Requires plotly and geolocation data
    # Placeholder for future implementation
    # Example: Plotting IP addresses on a map if geolocation data is available

# -------------------- ENDPOINT ANALYSIS --------------------

def endpoint_analysis(df: pd.DataFrame, output_dir: str):
    """
    Analyzes endpoint access patterns and generates visualizations.

    Parameters:
        df (pd.DataFrame): DataFrame for analysis
        output_dir (str): Directory to save visualizations

    Returns:
        None
    """
    logging.info("Starting endpoint analysis.")

    # Most Accessed Endpoints
    plt.figure(figsize=(12,6))
    top_endpoints = df['Endpoint'].value_counts().nlargest(20)
    sns.barplot(x=top_endpoints.values, y=top_endpoints.index)
    plt.title('Top 20 Most Accessed Endpoints')
    plt.xlabel('Number of Accesses')
    plt.ylabel('Endpoint')
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'endpoint_Top_20_Accessed.png'))
    plt.close()
    logging.info("Endpoint top 20 accessed bar chart saved.")

    # Endpoints Accessed during Anomalous vs Normal Logs
    plt.figure(figsize=(12,6))
    sns.countplot(data=df, y='Endpoint', hue='Anomalous', order=df['Endpoint'].value_counts().index)
    plt.title('Endpoints Accessed during Anomalous vs Normal Logs')
    plt.xlabel('Number of Accesses')
    plt.ylabel('Endpoint')
    plt.legend(title='Anomalous', labels=['Normal', 'Anomalous'])
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'endpoint_Accessed_Anomalous_vs_Normal.png'))
    plt.close()
    logging.info("Endpoint accessed during Anomalous vs Normal logs countplot saved.")

# -------------------- STATISTICAL SUMMARIES --------------------

def statistical_summaries(df: pd.DataFrame, summary_dir: str):
    """
    Generates statistical summaries and saves them as CSV files.

    Parameters:
        df (pd.DataFrame): DataFrame for analysis
        summary_dir (str): Directory to save summary CSVs

    Returns:
        None
    """
    logging.info("Generating statistical summaries.")

    # Descriptive Statistics for Numerical Features
    numerical_features = ['UserID']
    desc_stats = df[numerical_features].describe().transpose()
    desc_stats.to_csv(os.path.join(summary_dir, 'descriptive_statistics_numerical.csv'))
    logging.info("Descriptive statistics for numerical features saved.")

    # Descriptive Statistics for Timestamp
    timestamp_stats = {
        'Earliest Timestamp': [df['Timestamp'].min()],
        'Latest Timestamp': [df['Timestamp'].max()],
        'Mean Timestamp': [df['Timestamp'].mean()],
        'Median Timestamp': [df['Timestamp'].median()]
    }
    timestamp_stats_df = pd.DataFrame(timestamp_stats)
    timestamp_stats_df.to_csv(os.path.join(summary_dir, 'descriptive_statistics_timestamp.csv'), index=False)
    logging.info("Descriptive statistics for Timestamp saved.")

    # Frequency Counts for Categorical Features
    categorical_features = ['Role', 'HTTP_Method', 'Endpoint', 'User_Agent', 'HTTP_Response']
    for feature in categorical_features:
        freq_counts = df[feature].value_counts().reset_index()
        freq_counts.columns = [feature, 'Count']
        freq_counts.to_csv(os.path.join(summary_dir, f'frequency_counts_{feature}.csv'), index=False)
        logging.info(f"Frequency counts for {feature} saved.")

    # Percentage Distribution for Categorical Features
    for feature in categorical_features:
        freq_counts = df[feature].value_counts(normalize=True).reset_index()
        freq_counts.columns = [feature, 'Percentage']
        freq_counts.to_csv(os.path.join(summary_dir, f'percentage_distribution_{feature}.csv'), index=False)
        logging.info(f"Percentage distribution for {feature} saved.")

    # Anomaly Statistics
    anomaly_counts = df['Anomalous'].value_counts().reset_index()
    anomaly_counts.columns = ['Anomalous', 'Count']
    anomaly_counts['Percentage'] = df['Anomalous'].value_counts(normalize=True).values * 100
    anomaly_counts.to_csv(os.path.join(summary_dir, 'anomaly_statistics.csv'), index=False)
    logging.info("Anomaly statistics saved.")

    # Distribution of Anomalies across Roles, HTTP_Methods, and Endpoints
    for feature in ['Role', 'HTTP_Method', 'Endpoint']:
        anomaly_dist = pd.crosstab(df[feature], df['Anomalous'])
        anomaly_dist.to_csv(os.path.join(summary_dir, f'anomaly_distribution_{feature}.csv'))
        logging.info(f"Anomaly distribution across {feature} saved.")

# -------------------- BIAS AND OUTLIER DETECTION --------------------

def bias_and_outlier_detection(df: pd.DataFrame, summary_dir: str, output_dir: str):
    """
    Detects bias and outliers in the dataset.

    Parameters:
        df (pd.DataFrame): DataFrame for analysis
        summary_dir (str): Directory to save summary CSVs
        output_dir (str): Directory to save visualizations

    Returns:
        None
    """
    logging.info("Starting bias and outlier detection.")

    # a. Class Imbalance
    anomaly_ratio = df['Anomalous'].mean()
    with open(os.path.join(summary_dir, 'class_imbalance.txt'), 'w') as f:
        f.write(f"Anomalous Log Ratio: {anomaly_ratio:.4f}\n")
    logging.info("Class imbalance ratio saved.")

    # b. Role-Based Distribution
    role_counts = df['Role'].value_counts(normalize=True)
    role_counts.to_csv(os.path.join(summary_dir, 'role_based_distribution.csv'), header=['Proportion'])
    logging.info("Role-based distribution saved.")

    # c. Time-Based Bias
    logs_per_hour = df['Hour'].value_counts().sort_index()
    logs_per_hour.to_csv(os.path.join(summary_dir, 'logs_per_hour.csv'), header=['Count'])
    logging.info("Logs per hour distribution saved.")

    # d. Outlier Detection using Z-score for UserID
    df['UserID_Zscore'] = np.abs(stats.zscore(df['UserID']))
    outliers = df[df['UserID_Zscore'] > 3]
    outliers.to_csv(os.path.join(summary_dir, 'outliers_UserID.csv'), index=False)
    logging.info("Outliers based on UserID Z-score saved.")

    # e. Box Plot for UserID
    plt.figure(figsize=(10,6))
    sns.boxplot(x=df['UserID'])
    plt.title('Box Plot of UserID to Detect Outliers')
    plt.xlabel('UserID')
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'outlier_UserID_Boxplot.png'))
    plt.close()
    logging.info("Box plot for UserID outlier detection saved.")

    # f. Endpoint and Method Rare Combinations
    endpoint_method_counts = df.groupby(['Endpoint', 'HTTP_Method']).size().reset_index(name='Count')
    rare_combinations = endpoint_method_counts[endpoint_method_counts['Count'] < 10]
    rare_combinations.to_csv(os.path.join(summary_dir, 'rare_endpoint_method_combinations.csv'), index=False)
    logging.info("Rare endpoint and HTTP method combinations saved.")

# -------------------- DATA QUALITY CHECKS --------------------

def data_quality_checks(df: pd.DataFrame, summary_dir: str, output_dir: str):
    """
    Performs data quality checks and reports findings.

    Parameters:
        df (pd.DataFrame): DataFrame for analysis
        summary_dir (str): Directory to save summary reports
        output_dir (str): Directory to save any relevant visualizations

    Returns:
        None
    """
    logging.info("Starting data quality checks.")

    # a. Missing Values
    missing_values = df.isnull().sum()
    missing_values.to_csv(os.path.join(summary_dir, 'missing_values.csv'), header=['Missing_Count'])
    logging.info("Missing values per column saved.")

    # b. Duplicate Logs based on LogID
    duplicate_logs = df[df.duplicated(subset=['LogID'], keep=False)]
    duplicate_logs.to_csv(os.path.join(summary_dir, 'duplicate_logs.csv'), index=False)
    logging.info("Duplicate logs based on LogID saved.")

    # c. Consistency Checks between HTTP_Method and HTTP_Response
    # Example: If HTTP_Method is DELETE, HTTP_Response should not be 200
    inconsistent_logs = df[((df['HTTP_Method'] == 'DELETE') & (df['HTTP_Response'] == 200)) |
                           ((df['HTTP_Method'] == 'PUT') & (df['HTTP_Response'] == 404))]
    inconsistent_logs.to_csv(os.path.join(summary_dir, 'inconsistent_http_method_response.csv'), index=False)
    logging.info("Inconsistent HTTP_Method and HTTP_Response combinations saved.")

    # Visualization: Missing Values Heatmap
    plt.figure(figsize=(10,6))
    sns.heatmap(df.isnull(), cbar=False, cmap='viridis')
    plt.title('Missing Values Heatmap')
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'data_quality_Missing_Values_Heatmap.png'))
    plt.close()
    logging.info("Missing values heatmap saved.")

# -------------------- MAIN EDA FUNCTION --------------------

def perform_eda():
    """
    Main function to perform all EDA steps.

    Returns:
        None
    """
    # Load Data
    master_df, inference_df = load_data(MASTER_LOGS_PATH, INFERENCE_LOGS_PATH)

    # Initial Inspection
    initial_inspection(master_df, 'Master Logs')
    initial_inspection(inference_df, 'Inference Logs')

    # Univariate Analysis
    univariate_analysis(master_df, VISUALIZATIONS_DIR)

    # Bivariate Analysis
    bivariate_analysis(master_df, VISUALIZATIONS_DIR)

    # Time-Series Analysis
    time_series_analysis(master_df, VISUALIZATIONS_DIR)

    # Geographical Analysis
    geographical_analysis(master_df, VISUALIZATIONS_DIR)

    # Endpoint Analysis
    endpoint_analysis(master_df, VISUALIZATIONS_DIR)

    # Statistical Summaries
    statistical_summaries(master_df, SUMMARY_DIR)

    # Bias and Outlier Detection
    bias_and_outlier_detection(master_df, SUMMARY_DIR, VISUALIZATIONS_DIR)

    # Data Quality Checks
    data_quality_checks(master_df, SUMMARY_DIR, VISUALIZATIONS_DIR)

    logging.info("EDA completed successfully.")
    print("Exploratory Data Analysis completed. Check the 'eda_output' directory for results.")

# -------------------- ENTRY POINT --------------------

if __name__ == "__main__":
    perform_eda()
