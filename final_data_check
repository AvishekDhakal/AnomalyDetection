#!/usr/bin/env python3
"""
final_data_check.py

A script to perform final data integrity checks and a mini-EDA on
the already processed insider-threat dataset.

Usage Example:
    python final_data_check.py \
        --input_csv data/processed_logs.csv \
        --output_dir final_data_check_output

It will:
1. Load the dataset.
2. Verify data types, missing values, duplicates, and basic statistics.
3. Perform a mini-EDA with histograms, correlation matrix, anomaly distribution, etc.
4. Save visualizations and summaries to the specified output directory.

Author: [Your Name]
Date: YYYY-MM-DD
"""

import os
import argparse
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

def parse_arguments():
    parser = argparse.ArgumentParser(description="Final Data Integrity Check & Mini-EDA Script")
    parser.add_argument("--input_csv", type=str, default="data/processed_logs.csv",
                        help="Path to the processed CSV file.")
    parser.add_argument("--output_dir", type=str, default="final_data_check_output",
                        help="Directory to save output plots and summaries.")
    return parser.parse_args()

def load_data(input_csv: str) -> pd.DataFrame:
    """
    Loads the processed dataset into a pandas DataFrame.
    """
    df = pd.read_csv(input_csv)
    return df

def data_integrity_check(df: pd.DataFrame, output_dir: str):
    """
    Performs final data integrity checks:
    - Shape, column types, missing values, duplicates
    - Basic stats on numeric columns
    - Saves a summary text file in the output directory
    """
    os.makedirs(output_dir, exist_ok=True)
    
    summary_lines = []
    
    # 1. Shape
    summary_lines.append(f"Dataset Shape: {df.shape}\n")
    
    # 2. Column Types
    summary_lines.append("Column Types:\n")
    summary_lines.append(f"{df.dtypes}\n")
    
    # 3. Missing Values
    missing = df.isnull().sum()
    summary_lines.append("Missing Values:\n")
    summary_lines.append(f"{missing}\n")
    
    # 4. Duplicate Check (often on LogID)
    if "LogID" in df.columns:
        dupes = df[df.duplicated(subset=["LogID"], keep=False)]
        summary_lines.append(f"Number of Duplicate LogIDs: {len(dupes)}\n")
    else:
        summary_lines.append("LogID column not found; skipping duplicate check.\n")
    
    # 5. Basic Stats
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if numeric_cols:
        summary_lines.append("Basic Statistical Description (Numeric Columns):\n")
        summary_lines.append(f"{df[numeric_cols].describe()}\n")
    else:
        summary_lines.append("No numeric columns to describe.\n")
    
    # Write summary to file
    with open(os.path.join(output_dir, "final_data_integrity_summary.txt"), "w") as f:
        f.write("\n".join(summary_lines))
    
    print("[INFO] Data integrity check completed. Summary saved.")

def mini_eda(df: pd.DataFrame, output_dir: str):
    """
    Performs a mini-EDA:
    1. Distribution histograms for selected numeric columns
    2. Correlation heatmap
    3. Basic distribution of Anomalous vs Normal
    4. Any other quick checks (e.g., distribution of hour, is_after_hours, etc.)
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # 1. Distribution Histograms for Numeric Columns
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    # We'll skip some obviously binary columns if you prefer
    # e.g., 'Is_After_Hours', 'Is_Internal_IP', 'Anomalous' can be histograms if you want
    for col in numeric_cols:
        plt.figure(figsize=(6,4))
        sns.histplot(df[col], bins=30, kde=True)
        plt.title(f"Distribution of {col}")
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, f"hist_{col}.png"))
        plt.close()
    
    # 2. Correlation Heatmap
    if len(numeric_cols) > 1:
        corr = df[numeric_cols].corr()
        plt.figure(figsize=(10,8))
        sns.heatmap(corr, annot=False, cmap='viridis')
        plt.title("Correlation Heatmap (Numeric Features)")
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, "correlation_heatmap.png"))
        plt.close()
    
    # 3. Basic Distribution of Anomalous vs Normal
    if "Anomalous" in df.columns:
        anomaly_counts = df["Anomalous"].value_counts(dropna=False)
        
        plt.figure(figsize=(5,4))
        sns.barplot(x=anomaly_counts.index.astype(str), y=anomaly_counts.values)
        plt.title("Distribution of Anomalous vs Normal")
        plt.xlabel("Anomalous")
        plt.ylabel("Count")
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, "anomalous_distribution.png"))
        plt.close()

        # Print ratio in console
        anomaly_ratio = anomaly_counts.get(1, 0) / df.shape[0]
        print(f"[INFO] Anomaly Ratio: {anomaly_ratio:.4f} ({anomaly_counts.get(1, 0)} / {df.shape[0]} logs)")
    
    # 4. Example: distribution of 'Hour' or 'DayOfWeek' if they exist
    for col in ["Hour", "DayOfWeek"]:
        if col in df.columns:
            plt.figure(figsize=(6,4))
            sns.countplot(x=col, data=df)
            plt.title(f"Distribution of {col}")
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, f"count_{col}.png"))
            plt.close()
    
    print("[INFO] Mini-EDA completed. Plots saved.")

def main():
    args = parse_arguments()
    
    # 1. Load Data
    df = load_data(args.input_csv)
    print(f"[INFO] Loaded {df.shape[0]} rows and {df.shape[1]} columns from {args.input_csv}")
    
    # 2. Data Integrity Check
    data_integrity_check(df, args.output_dir)
    
    # 3. Mini EDA
    mini_eda(df, args.output_dir)
    
    print("[INFO] Final data check and mini-EDA finished successfully.")

if __name__ == "__main__":
    main()
